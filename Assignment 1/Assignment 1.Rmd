---
title: "Assignment 1"
author: "Álvaro y Pablo"
date: "10/6/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preprocesamiento: Carga datos y librerias

Cargamos las librerias a usar:

```{r, message=FALSE,warning=FALSE}
library(tidyverse)
library(GGally)
library(MLTools)
library(caret)
library(ROCR)
```

Cargamos los datos:

```{r}
datos <- read.table("./data/Diabetes.csv", sep = ";", header = TRUE)
```




# Análisis Exploratorio


Resumen general del datasets
```{r}
str(datos)
summary(datos)
```

Realizamos algunos cambios en el dataset
```{r}
datos <- datos %>%
  mutate(DIABETES = ifelse(DIABETES == 1, "Si", "No"))
datos <- datos %>%
  mutate(DIABETES = as.factor(DIABETES))
str(datos)
```

Vemos que no hay datos nulos y por lo tanto trabajaremos con todas las filas que hemos cargado

Antes de iniciar con todos los gráficos veamos una representación de todos con todos:

```{r message=FALSE, warning=FALSE}
ggpairs(datos, aes(color = DIABETES))
```

Da la sensación de existir bastantes valores atipicos, veamoslo:

```{r}
boxplot_glucosa <- boxplot(datos$GLUCOSE)
boxplot_glucosa$out
```
Una persona con nivel de glucosa 0 es imposible que este viva, por ello vamos a eliminar dichas observaciones.

```{r}
datos2 <- datos
datos2 <- datos2[datos2$GLUCOSE > 0, ]
```


```{r}
datos3 <- datos
datos3$GLUCOSE <- ifelse(datos3$GLUCOSE == 0, median(datos3$GLUCOSE), datos3$GLUCOSE)
```

```{r}
datos4 <- datos
datos4$GLUCOSE <- ifelse(datos4$GLUCOSE == 0, mean(datos4$GLUCOSE), datos4$GLUCOSE)
```


Comprobamos mirando un modelo sencillo cual es la mejor opción:


```{r}
trainIndex <- createDataPartition(datos2$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR2 <- datos2[trainIndex,]
fTS2 <- datos2[-trainIndex,]
fTR_eval2 <- fTR2
fTS_eval2 <- fTS2

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit2 <- train(form = DIABETES ~ GLUCOSE,
                    data = fTR2,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit2

```


```{r}
trainIndex <- createDataPartition(datos3$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ GLUCOSE,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}
trainIndex <- createDataPartition(datos4$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ GLUCOSE,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```

El modelo que mejor nos ha salido es en el que cambiamos los valores de 0 de la glucosa por la mediana.

Veamos ahora la presión sanguinea:

```{r}
boxplot_blood <- boxplot(datos$BLOODPRESS)

```
Volvemos a realizar lo mismo que en el caso anterior:

```{r}
datos2 <- datos2[datos2$BLOODPRESS > 0, ]
```


```{r}
datos3$BLOODPRESS <- ifelse(datos3$BLOODPRESS == 0, median(datos3$BLOODPRESS), datos3$BLOODPRESS)
```

```{r}
datos4$BLOODPRESS <- ifelse(datos4$BLOODPRESS == 0, mean(datos4$BLOODPRESS), datos4$BLOODPRESS)
```

```{r}
trainIndex <- createDataPartition(datos2$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR2 <- datos2[trainIndex,]
fTS2 <- datos2[-trainIndex,]
fTR_eval2 <- fTR2
fTS_eval2 <- fTS2

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit2 <- train(form = DIABETES ~ BLOODPRESS,
                    data = fTR2,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit2
```


```{r}
trainIndex <- createDataPartition(datos3$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ BLOODPRESS,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}
trainIndex <- createDataPartition(datos4$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ BLOODPRESS,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```

Hacemos lo mismo con el bodymassindex:

```{r}
datos2 <- datos2[datos2$BODYMASSINDEX > 0, ]
```


```{r}
datos3$BODYMASSINDEX <- ifelse(datos3$BODYMASSINDEX == 0, median(datos3$BODYMASSINDEX), datos3$BODYMASSINDEX)
```

```{r}
datos4$BODYMASSINDEX <- ifelse(datos4$BODYMASSINDEX == 0, mean(datos4$BODYMASSINDEX), datos4$BODYMASSINDEX)
```

Y entrenamos modelos:

```{r}
trainIndex <- createDataPartition(datos2$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR2 <- datos2[trainIndex,]
fTS2 <- datos2[-trainIndex,]
fTR_eval2 <- fTR2
fTS_eval2 <- fTS2

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit2 <- train(form = DIABETES ~ BODYMASSINDEX,
                    data = fTR2,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit2
```


```{r}
trainIndex <- createDataPartition(datos3$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ BODYMASSINDEX,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}
trainIndex <- createDataPartition(datos4$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ BODYMASSINDEX,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```

Hacemos lo mismo con el insulina, lo unico que en este caso no contemplamos eliminar los datos que son 0 ya que representan gran cantidad de nuestro dataset.

```{r}
datos3$INSULIN <- ifelse(datos3$INSULIN == 0, median(datos3$INSULIN), datos3$INSULIN)
```

```{r}
datos4$INSULIN <- ifelse(datos4$INSULIN == 0, mean(datos4$INSULIN), datos4$INSULIN)
```

Y entrenamos modelos:


```{r}
trainIndex <- createDataPartition(datos3$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ INSULIN,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}
trainIndex <- createDataPartition(datos4$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ INSULIN,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```


Por último nos queda ver la variable Sthickness que era la otra que tenía datos atípicos a simple vista:

```{r}
boxplot_sthick <- boxplot(datos$SKINTHICKNESS)
```

Aunque no se vean tambien existen valores atípicos, que son erroneos, como es el caso del 0.

Por tanto nos encontramos en el mismo caso:

```{r}
datos2 <- datos2[datos2$SKINTHICKNESS > 0 & datos2$SKINTHICKNESS < 90, ]
```


```{r}
datos3$SKINTHICKNESS <- ifelse(datos3$SKINTHICKNESS == 0 , median(datos3$SKINTHICKNESS), datos3$SKINTHICKNESS)
datos3$SKINTHICKNESS <- ifelse(datos3$SKINTHICKNESS > 90 , median(datos3$SKINTHICKNESS), datos3$SKINTHICKNESS)
```

```{r}
datos4$SKINTHICKNESS <- ifelse(datos4$SKINTHICKNESS == 0, mean(datos4$SKINTHICKNESS), datos4$SKINTHICKNESS)
datos4$SKINTHICKNESS <- ifelse(datos4$SKINTHICKNESS > 90, mean(datos4$SKINTHICKNESS), datos4$SKINTHICKNESS)
```

Y entrenamos modelos:

```{r}
trainIndex <- createDataPartition(datos2$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR2 <- datos2[trainIndex,]
fTS2 <- datos2[-trainIndex,]
fTR_eval2 <- fTR2
fTS_eval2 <- fTS2

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit2 <- train(form = DIABETES ~ SKINTHICKNESS,
                    data = fTR2,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit2
```

```{r}
trainIndex <- createDataPartition(datos3$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ SKINTHICKNESS,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}
trainIndex <- createDataPartition(datos4$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ SKINTHICKNESS,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```

Hacemos todos los cambios que hemos ido deciciendo sobre el dataset original:

```{r}
datos$GLUCOSE <- ifelse(datos$GLUCOSE == 0, median(datos$GLUCOSE), datos$GLUCOSE)
datos$BODYMASSINDEX <- ifelse(datos$BODYMASSINDEX == 0, median(datos$BODYMASSINDEX), datos$BODYMASSINDEX)
datos$BLOODPRESS <- ifelse(datos$BLOODPRESS == 0, median(datos$BLOODPRESS), datos$BLOODPRESS)
datos$INSULIN <- ifelse(datos$INSULIN == 0, median(datos$INSULIN), datos$INSULIN)
datos$INSULIN <- ifelse(datos$INSULIN > 600, median(datos$INSULIN), datos$INSULIN)
datos$SKINTHICKNESS <- ifelse(datos$SKINTHICKNESS == 0, median(datos$SKINTHICKNESS), datos$SKINTHICKNESS)
datos$SKINTHICKNESS <- ifelse(datos$SKINTHICKNESS > 90, median(datos$SKINTHICKNESS), datos$SKINTHICKNESS)
```

Y volvemos a hacer un EDA:

```{r}
ggpairs(datos, aes(color = DIABETES))
```
Todo bien, todo correcto.


# Identification and fitting process of classification models

Lo primero que hacemos antes de realizar ningún modelo será informarnos acerca del tema, con la finalidad de conocer cuales son las variables que más afectan a la diabetes.

Tras leer numerosos articulos podemos llegar a la conclusion que las variables que mas afectan a nuestra variable respuesta son tener obesidad, edad, presión arterial alta, antecedentes familiares y altos niveles de glucosa. 

Para comenzar con el modelo lo que haremos será dividir nuestra muestra en 2 grupos, el de entrenamiento y el de test. La proporción con la que trabajaremos será de un 80-20.












